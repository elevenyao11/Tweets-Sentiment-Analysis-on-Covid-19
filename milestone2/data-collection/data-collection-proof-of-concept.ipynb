{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data-collection-proof-of-concept.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YCMX_u99PMB"
      },
      "source": [
        "# Data Collection Proof-of-Concept\n",
        "##  -- Mega-Cov Hydrator\n",
        "\n",
        "The Google Colab notebook shows to to hydrate an Mega-cov archive.\n",
        "\n",
        "You need to run this on Google Colab to get tweet metadata including its text.\n",
        "\n",
        "The demo below hydrates 1 month of Mega-Cov data, filter it by Country == Canada & Language == English. \n",
        "\n",
        "It gets a total of 292 tweets in the end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjHK1NVR_cM6"
      },
      "source": [
        "# Initializaition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acjblhY9_ghH"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "%pip install twarc\n",
        "%pip install jsonlines\n",
        "# %pip install wget\n",
        "%pip install jsonlines\n",
        "clear_output()\n",
        "\n",
        "import os\n",
        "from google.colab import drive \n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "# import wget\n",
        "import json\n",
        "import jsonlines\n",
        "import tqdm\n",
        "from twarc import Twarc\n"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtnFLQK6_hzx"
      },
      "source": [
        "# shared archive -> \"local\" archive\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKit86WE-N-8"
      },
      "source": [
        "Follow these steps:\n",
        "\n",
        "1. Go to your Googe Drive\n",
        "2. Create a folder named `MegaCov` \n",
        "3. Locate a Mega-cov archive file from [here](https://github.com/UBC-NLP/megacov/tree/master/tweet_ids) \n",
        "4. Add it to the `MegaCov` folder you just created in your Google Drive.\n",
        "\n",
        "\n",
        "Now run the cell below.\n",
        "\n",
        "When prompted, grant acess."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJjCtkzm9iol",
        "outputId": "e995904b-b633-4379-d184-b084f5ac87ff"
      },
      "source": [
        "\n",
        "drive.mount('/content/gdrive')\n",
        "working_directory = 'My Drive/MegaCov'\n",
        "\n",
        "wd=\"/content/gdrive/\"+working_directory\n",
        "os.chdir(wd)\n",
        "\n",
        "dirpath = os.getcwd()\n",
        "print(\"current directory is : \" + dirpath)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "current directory is : /content/gdrive/My Drive/MegaCov\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea2BgxtP4PyP"
      },
      "source": [
        "# archive -> json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vogjrpZ9-zku"
      },
      "source": [
        "path = os.path.join(wd, \"tweet_ids\") \n",
        "if not os.path.exists('./tweet_ids'):\n",
        "  try:\n",
        "    os.makedirs(path)\n",
        "    # os.chdir('tweet_ids')\n",
        "  except:\n",
        "    pass"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bn_WJUOTCR9Q"
      },
      "source": [
        "#!pwd"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MZ7djiHCetJ",
        "outputId": "790f9ca2-81b0-4939-8146-b41b6e597b5f"
      },
      "source": [
        "!tar -xvf  'MagaCOV_May_2020.tar.gz' -C './tweet_ids'\n",
        "#todo unzip in batch *.tar.gz files"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5_2020_1.json\n",
            "5_2020_2.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L3cIGXD4Wvq"
      },
      "source": [
        "#  json  + filter -> tweet_ids"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6msB_MGYD1c3"
      },
      "source": [
        "os.chdir('tweet_ids')\n",
        "\n",
        "file = '5_2020_2.json'\n"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAazCk5Z03cq"
      },
      "source": [
        "twt_ids = []\n",
        "\n",
        "#todo iterate over *.json in folder\n",
        "\n",
        "with jsonlines.open(file) as reader:\n",
        "    for obj in reader:\n",
        "        #  filter by country and by Language:\n",
        "        if obj['country'] == 'Canada' and obj['LangID_tool'] == 'en':\n",
        "            twt_ids.append(obj['tweet_id'])"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6t5xWWfwc_x",
        "outputId": "8f49f431-eacf-4ce5-968c-c7254d984e81"
      },
      "source": [
        "assert len(twt_ids) == 292\n",
        "print('pass')"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pass\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4rg9frv-EBZ",
        "outputId": "e247fb1d-461a-4c80-fd9d-a0cb49a5ea3c"
      },
      "source": [
        "assert twt_ids[0] == 1258551281041330176\n",
        "print('pass')"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pass\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnTFBt78592l"
      },
      "source": [
        "# tweet_ids -> txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Afa5dD_y6E_k"
      },
      "source": [
        "#@title Enter ID output file {run: \"auto\"}\n",
        "os.chdir(wd)\n",
        "final_tweet_ids_filename = \"ids_CA_2020May_2.txt\" #@param {type: \"string\"}\n",
        "# The set of IDs is stored in this file.\n",
        "with open(final_tweet_ids_filename, \"w+\") as f:\n",
        "    for id in twt_ids:\n",
        "        f.write('%s\\n' % id)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDEcRpe84_Ex"
      },
      "source": [
        "# Hydrate "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvGiy9GEAHHO"
      },
      "source": [
        "## --using Twarc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRqkjBkPD-C6"
      },
      "source": [
        "#@title Insert API Keys here { run : \"auto\"}\n",
        "from twarc import Twarc\n",
        "\n",
        "# These keys are received after applying for a twitter developer account\n",
        "\n",
        "consumer_key = \"\" #@param {type:\"string\"}\n",
        "consumer_secret = \"\" #@param {type:\"string\"}\n",
        "access_token = \"\" #@param {type:\"string\"}\n",
        "access_token_secret = \"\" #@param {type:\"string\"}\n",
        "\n",
        "t = Twarc(consumer_key, consumer_secret, access_token, access_token_secret)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOW5TDdUEPTh"
      },
      "source": [
        "#@title Set up Directory { run: \"auto\"}\n",
        "input_filename_tweet_ids = \"ids_CA_2020May_2.txt\" #@param {type: \"string\"}\n",
        "output_filename = \"all-metadata-Canada-Engish-2020May_2.csv\" #@param {type: \"string\"}"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AHSRPAT0TUd",
        "outputId": "db30222b-2a3e-4ba4-a289-08ff87c665c6"
      },
      "source": [
        "# Stores hydrated tweets here as jsonl objects\n",
        "# Contains one json object per line\n",
        "output_json_filename = output_filename[:output_filename.index(\".\")] + \".jsonl\"\n",
        "ids = []\n",
        "with open(final_tweet_ids_filename, \"r\") as ids_file:\n",
        "    ids = ids_file.read().split()\n",
        "hydrated_tweets = []\n",
        "ids_to_hydrate = set(ids)\n",
        "\n",
        "# Looks at the output file for already hydrated tweets\n",
        "if os.path.isfile(output_json_filename):\n",
        "    with jsonlines.open(output_json_filename, \"r\") as reader:\n",
        "        for i in reader.iter(type=dict, skip_invalid=True):\n",
        "            # These tweets have already been hydrated. So remove them from ids_to_hydrate\n",
        "            hydrated_tweets.append(i)\n",
        "            ids_to_hydrate.remove(i[\"id_str\"])\n",
        "print(\"Total IDs: \" + str(len(ids)) + \", IDs to hydrate: \" + str(len(ids_to_hydrate)))\n",
        "print(\"Hydrated: \" + str(len(hydrated_tweets)))\n",
        "\n",
        "count = len(hydrated_tweets)\n",
        "start_index = count # The index from where tweets haven't been saved to the output_json_file\n",
        "# Stores hydrated tweets to output_json_file every num_save iterations.\n",
        "num_save  = 1000\n",
        "\n",
        "# Now, use twarc and start hydrating\n",
        "for tweet in t.hydrate(ids_to_hydrate):\n",
        "    hydrated_tweets.append(tweet)\n",
        "    count += 1\n",
        "    # If num_save iterations have passed,\n",
        "    if (count % num_save) == 0:\n",
        "        # Open the output file\n",
        "        # NOTE: Even if the code stops during IO, only tweets from the current iteration are lost.\n",
        "        # Older tweets are preserved as the file is written in append mode.\n",
        "        with jsonlines.open(output_json_filename, \"a\") as writer:\n",
        "            print(\"Started IO\")\n",
        "            # Now write the tweets from start_index. The other tweets don't have to be written\n",
        "            # as they were already written in a previous iteration or run.\n",
        "            for hydrated_tweet in hydrated_tweets[start_index:]:\n",
        "                writer.write(hydrated_tweet)\n",
        "            print(\"Finished IO\")\n",
        "        print(\"Saved \" + str(count) + \" hydrated tweets.\")\n",
        "        # Now, since everything has been written. Reset start_index\n",
        "        start_index = count\n",
        "# There might be tweets unwritten in the last iteration if the count is not a multiple of num_tweets.\n",
        "# In that case, just write out the remainder of tweets.\n",
        "if count != start_index:\n",
        "    print(\"Here with start_index\", start_index)\n",
        "    with jsonlines.open(output_json_filename, \"a\") as writer:\n",
        "        for hydrated_tweet in hydrated_tweets[start_index:]:\n",
        "           writer.write(hydrated_tweet)   "
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total IDs: 292, IDs to hydrate: 292\n",
            "Hydrated: 0\n",
            "Here with start_index 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkfhAuHH6q4g"
      },
      "source": [
        "# Convert jsonl to csv\n",
        "import csv, jsonlines\n",
        "output_json_filename = output_filename[:output_filename.index(\".\")] + \".jsonl\"\n",
        "# These are the column name that are selected to be stored in the csv\n",
        "keyset = [\"created_at\", \"id\", \"id_str\", \"full_text\", \"source\", \"truncated\", \"in_reply_to_status_id\",\n",
        "          \"in_reply_to_status_id_str\", \"in_reply_to_user_id\", \"in_reply_to_user_id_str\", \n",
        "          \"in_reply_to_screen_name\", \"user\", \"coordinates\", \"place\", \"quoted_status_id\",\n",
        "          \"quoted_status_id_str\", \"is_quote_status\", \"quoted_status\", \"retweeted_status\", \n",
        "          \"quote_count\", \"reply_count\", \"retweet_count\", \"favorite_count\", \"entities\", \n",
        "          \"extended_entities\", \"favorited\", \"retweeted\", \"possibly_sensitive\", \"filter_level\", \n",
        "          \"lang\", \"matching_rules\", \"current_user_retweet\", \"scopes\", \"withheld_copyright\", \n",
        "          \"withheld_in_countries\", \"withheld_scope\", \"geo\", \"contributors\", \"display_text_range\",\n",
        "          \"quoted_status_permalink\"]\n",
        "hydrated_tweets = []\n",
        "# Reads the current tweets\n",
        "with jsonlines.open(output_json_filename, \"r\") as reader:\n",
        "    for i in reader.iter(type=dict, skip_invalid=True):\n",
        "        hydrated_tweets.append(i)\n",
        "# Writes them out\n",
        "with  open(output_filename, \"w+\") as output_file:\n",
        "    d = csv.DictWriter(output_file, keyset)\n",
        "    d.writeheader()\n",
        "    d.writerows(hydrated_tweets)"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YvxP0Hk-kbQ"
      },
      "source": [
        "Now the csv should be in MegaCov folder."
      ]
    }
  ]
}